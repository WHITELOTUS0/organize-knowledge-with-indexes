{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring The Role of LangChain's Indexes and Retrievers\n",
        "## Introduction\n",
        "\n",
        "In LangChain, indexes and retrievers play a crucial role in structuring documents and fetching relevant data for LLMs.  We will explore some of the advantages and disadvantages of using document based LLMs (i.e., LLMs that leverage relevant pieces of documents inside their prompts), with a particular focus on the role of indexes and retrievers.\n",
        "\n",
        "An `index` is a powerful data structure that meticulously organizes and stores documents to enable efficient searching, while a `retriever` harnesses the index to locate and return pertinent documents in response to user queries. Within LangChain, the primary index types are centered on vector databases, with embeddings-based indexes being the most prevalent.\n",
        "\n",
        "Retrievers focus on extracting relevant documents to merge with prompts for language models. A retriever exposes a `get_relevant_documents` method, which accepts a query string as input and returns a list of related documents.\n",
        "\n",
        "Here we use the TextLoader class to load a text file. Remember to install the required packages with the following command:"
      ],
      "metadata": {
        "id": "U254kWXVIcU3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5W1CBcwfHhni",
        "outputId": "b6b49eda-4553-4da4-8288-6b7e031dbb58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.3/454.3 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m618.7/618.7 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.7/17.7 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for deeplake (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "%pip install -qU langchain-text-splitters\n",
        "%pip install -qU langchain-openai\n",
        "%pip install -qU langchain-community\n",
        "%pip install -q deeplake==3.9.27 tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "# text to write to a local file\n",
        "# taken from https://www.theverge.com/2023/3/14/23639313/google-ai-language-model-palm-api-challenge-openai\n",
        "text = \"\"\"Google opens up its AI language model PaLM to challenge OpenAI and GPT-3\n",
        "Google is offering developers access to one of its most advanced AI language models: PaLM.\n",
        "The search giant is launching an API for PaLM alongside a number of AI enterprise tools\n",
        "it says will help businesses “generate text, images, code, videos, audio, and more from\n",
        "simple natural language prompts.”\n",
        "\n",
        "PaLM is a large language model, or LLM, similar to the GPT series created by OpenAI or\n",
        "Meta’s LLaMA family of models. Google first announced PaLM in April 2022. Like other LLMs,\n",
        "PaLM is a flexible system that can potentially carry out all sorts of text generation and\n",
        "editing tasks. You could train PaLM to be a conversational chatbot like ChatGPT, for\n",
        "example, or you could use it for tasks like summarizing text or even writing code.\n",
        "(It’s similar to features Google also announced today for its Workspace apps like Google\n",
        "Docs and Gmail.)\n",
        "\"\"\"\n",
        "\n",
        "# write text to local file\n",
        "with open(\"my_file.txt\", \"w\") as file:\n",
        "    file.write(text)\n",
        "\n",
        "# use TextLoader to load text from local file\n",
        "loader = TextLoader(\"my_file.txt\")\n",
        "docs_from_file = loader.load()\n",
        "\n",
        "print(len(docs_from_file))\n",
        "# 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKxVuQJdJD5A",
        "outputId": "cfb92d4e-4705-42a6-b271-ad362f276f9d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we use CharacterTextSplitter to split the docs into texts."
      ],
      "metadata": {
        "id": "ZH2dIlVdJTTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "# create a text splitter\n",
        "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
        "\n",
        "# split documents into chunks\n",
        "docs = text_splitter.split_documents(docs_from_file)\n",
        "\n",
        "print(len(docs))\n",
        "# 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dD8V2nKJURD",
        "outputId": "6814d2cb-97f7-40e5-dd2f-8427b53e2726"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_text_splitters.base:Created a chunk of size 373, which is longer than the specified 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These embeddings allow us to effectively search for documents or portions of documents that relate to our query by examining their semantic similarities."
      ],
      "metadata": {
        "id": "oVIftimYKZIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get the API key from Colab's userdata\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Set it as an environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "\n",
        "activeloop_token = userdata.get('ACTIVELOOP_TOKEN')\n",
        "\n",
        "# Set it as an environment variable\n",
        "os.environ[\"ACTIVELOOP_TOKEN\"] = activeloop_token\n",
        "\n",
        "activeloop_org_id = userdata.get('ACTIVELOOP_ORG_ID')\n",
        "\n",
        "os.environ[\"ACTIVELOOP_ORG_ID\"] = activeloop_org_id\n",
        "\n",
        "# Before executing the following code, make sure to have\n",
        "# your OpenAI key saved in the “OPENAI_API_KEY” environment variable.\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5TWfRArJxSV",
        "outputId": "60b38c55-6882-4c16-9353-975e9390bcba"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-622dc7fbeb6b>:22: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
            "  embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s create an instance of a Deep Lake dataset."
      ],
      "metadata": {
        "id": "LUAwsTC0Kevn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import DeepLake\n",
        "\n",
        "# Before executing the following code, make sure to have your\n",
        "# Activeloop key saved in the “ACTIVELOOP_TOKEN” environment variable.\n",
        "\n",
        "# create Deep Lake dataset\n",
        "# TODO: use your organization id here. (by default, org id is your username)\n",
        "my_activeloop_org_id = activeloop_org_id\n",
        "my_activeloop_dataset_name = \"langchain_course_indexers_retrievers\"\n",
        "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
        "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
        "\n",
        "# add documents to our Deep Lake dataset\n",
        "db.add_documents(docs)"
      ],
      "metadata": {
        "id": "fY7mFK3gKfhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we are adding text documents to the dataset. However, being Deep Lake multimodal, we could have also added images to it, specifying an image embedder model. This could be useful for searching images according to a text query or using an image as a query (and thus looking for similar images).\n",
        "\n",
        "As datasets become bigger, storing them in local memory becomes less manageable. In this example, we could have also used a local vector store, as we are uploading only two documents. However, in a typical production scenario, thousands or millions of documents could be used and accessed from different programs, thus having the need for a centralized cloud dataset.\n",
        "\n",
        "Back to the code example of this lesson. Next, we create a retriever."
      ],
      "metadata": {
        "id": "hKG_PuqdLLWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create retriever from db\n",
        "retriever = db.as_retriever()"
      ],
      "metadata": {
        "id": "HkB3lTjJLMGp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we have the retriever, we can start with question-answering."
      ],
      "metadata": {
        "id": "1BWfh1PPLWfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# create a retrieval chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "\tllm=OpenAI(model=\"gpt-3.5-turbo-instruct\"),\n",
        "\tchain_type=\"stuff\",\n",
        "\tretriever=retriever\n",
        ")"
      ],
      "metadata": {
        "id": "950n8KaHLXi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can query our document that is an about specific topic that can be found in the documents."
      ],
      "metadata": {
        "id": "8GDIzPuYLk6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How Google plans to challenge OpenAI?\"\n",
        "response = qa_chain.run(query)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-z_59ewNLlxc",
        "outputId": "1b57808c-32f6-42fb-8cf3-aa178a514e2c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-4133f5f974df>:2: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = qa_chain.run(query)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Google plans to challenge OpenAI by opening up its AI language model PaLM to developers and launching an API for PaLM alongside other AI enterprise tools.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A Potential Problem\n",
        "This method has a downside: you might not know how to get the right documents later when storing data. In the Q&A example, we cut the text into equal parts, causing both useful and useless text to show up when a user asks a question.\n",
        "\n",
        "Including unrelated information in the LLM prompt is detrimental because:\n",
        "\n",
        "- It can divert the LLM's focus from pertinent details.\n",
        "- It occupies valuable space that could be utilized for more relevant information.\n",
        "\n",
        "### Possible Solution\n",
        "A `DocumentCompressor` abstraction has been introduced to address this issue, allowing compress_documents on the retrieved documents.\n",
        "\n",
        "The `ContextualCompressionRetriever` is a wrapper around another retriever in LangChain. It takes a base retriever and a `DocumentCompressor` and automatically compresses the retrieved documents from the base retriever. This means that only the most relevant parts of the retrieved documents are returned, given a specific query.\n",
        "\n",
        "A popular compressor choice is the `LLMChainExtractor`, which uses an LLMChain to extract only the statements relevant to the query from the documents. To improve the retrieval process, a ContextualCompressionRetriever is used, wrapping the base retriever with an LLMChainExtractor. The LLMChainExtractor iterates over the initially returned documents and extracts only the content relevant to the query.\n",
        "\n",
        "Here's an example of how to use `ContextualCompressionRetriever` with `LLMChainExtractor`:"
      ],
      "metadata": {
        "id": "QiVOnHtbPW9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "\n",
        "# create GPT3 wrapper\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
        "\n",
        "# create compressor for the retriever\n",
        "compressor = LLMChainExtractor.from_llm(llm)\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "\tbase_compressor=compressor,\n",
        "\tbase_retriever=retriever\n",
        ")"
      ],
      "metadata": {
        "id": "WUzEsXvfPswW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we have created the compression_retriever, we can use it to retrieve the compressed relevant documents to a query."
      ],
      "metadata": {
        "id": "NQn959SCP69N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs = compression_retriever.get_relevant_documents(\n",
        "\t\"How Google plans to challenge OpenAI?\"\n",
        ")\n",
        "print(retrieved_docs[0].page_content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaf90SNdP9zS",
        "outputId": "f9efec70-44a1-4067-8c40-9b8b4c869efe"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-320c822cb562>:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  retrieved_docs = compression_retriever.get_relevant_documents(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google opens up its AI language model PaLM to challenge OpenAI and GPT-3\n",
            "Google is offering developers access to one of its most advanced AI language models: PaLM.\n",
            "The search giant is launching an API for PaLM alongside a number of AI enterprise tools\n"
          ]
        }
      ]
    }
  ]
}