# Intro to Keeping Knowledge Organized with Indexes

Regardless of the chosen model or prompt formulation, language models have inherent limitations. These models have a cut-off date for the training process, which means they typically lack access to trending news and the latest developments. This limitation can result in the models providing responses that may not be factually accurate and potentially hallucinating information.

In this module, we will delve into techniques that enable us to provide accurate context to language models, enhancing their ability to answer questions effectively. Additional context can be sourced from various channels such as databases, URLs, or different file types. Several preprocessing steps are necessary to facilitate this process. These include utilizing splitters to ensure the content's length falls within the model's input window size and converting text into embedding vectors, which aids in identifying contextually similar resources.
